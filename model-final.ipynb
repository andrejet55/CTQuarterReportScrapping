{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/anaconda3/lib/python3.12/site-packages (0.18.7)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (2.32.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (0.70.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (4.67.1)\n",
      "Requirement already satisfied: regex in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2024.11.6)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.5.2)\n",
      "Requirement already satisfied: seqeval in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: tensorboardx in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (0.20.4)\n",
      "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (0.18.7)\n",
      "Requirement already satisfied: streamlit in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.40.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (2024.8.30)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (3.11.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from pandas->simpletransformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from pandas->simpletransformers) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from pandas->simpletransformers) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (6.0.0)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (6.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (1.68.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (3.7)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (3.1.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (1.14.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.18.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n"
     ]
    }
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (0.70.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (4.67.1)\n",
      "Requirement already satisfied: regex in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2024.11.6)\n",
      "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (4.46.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.5.2)\n",
      "Requirement already satisfied: seqeval in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: tensorboardx in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (0.20.4)\n",
      "Requirement already satisfied: wandb>=0.10.32 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (0.18.7)\n",
      "Requirement already satisfied: streamlit in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (1.40.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from transformers>=4.31.0->simpletransformers) (0.4.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (1.3.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from wandb>=0.10.32->simpletransformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from requests->simpletransformers) (2024.8.30)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from datasets->simpletransformers) (3.11.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from pandas->simpletransformers) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from pandas->simpletransformers) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from pandas->simpletransformers) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from scikit-learn->simpletransformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from scikit-learn->simpletransformers) (3.5.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (5.5.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (11.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (6.0.0)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from streamlit->simpletransformers) (6.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (1.68.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (3.7)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from tensorboard->simpletransformers) (3.1.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.23.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers) (1.14.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from aiohttp->datasets->simpletransformers) (1.18.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.11)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (3.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\andrea fs\\onedrive\\lambton college\\second term\\new_nlp\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import json\n",
    "import logging\n",
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "with open(r\"train.json\", \"r\") as read_file:\n",
    "    train = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "with open(r\"test.json\", \"r\") as read_file:\n",
    "    test = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check test position of answers\n",
    "for entry in train:\n",
    "    context = entry[\"context\"]\n",
    "    for qa in entry[\"qas\"]:\n",
    "        for answer in qa[\"answers\"]:\n",
    "            answer_text = answer[\"text\"]\n",
    "            start = answer[\"answer_start\"]\n",
    "            if context[start:start + len(answer_text)] != answer_text:\n",
    "                print(f\"Mismatch in context: '{context[start:start + len(answer_text)]}' vs '{answer_text}'\")\n",
    "                # print the correct start position\n",
    "                print(f\"Correct start position: {context.find(answer_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure no qas or answers arrays are empty.\n",
    "for entry in train:\n",
    "    if not entry[\"qas\"]:\n",
    "        print(f\"Empty 'qas' in context: {entry['context']}\")\n",
    "    for qa in entry[\"qas\"]:\n",
    "        if not qa[\"answers\"]:\n",
    "            print(f\"Empty 'answers' for question: {qa['question']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type=\"bert\"\n",
    "model_name= \"bert-base-cased\"\n",
    "if model_type == \"bert\":\n",
    "    model_name = \"bert-base-cased\"\n",
    "\n",
    "elif model_type == \"roberta\":\n",
    "    model_name = \"roberta-base\"\n",
    "\n",
    "elif model_type == \"distilbert\":\n",
    "    model_name = \"distilbert-base-cased\"\n",
    "\n",
    "elif model_type == \"distilroberta\":\n",
    "    model_type = \"roberta\"\n",
    "    model_name = \"distilroberta-base\"\n",
    "\n",
    "elif model_type == \"electra-base\":\n",
    "    model_type = \"electra\"\n",
    "    model_name = \"google/electra-base-discriminator\"\n",
    "\n",
    "elif model_type == \"electra-small\":\n",
    "    model_type = \"electra\"\n",
    "    model_name = \"google/electra-small-discriminator\"\n",
    "\n",
    "elif model_type == \"xlnet\":\n",
    "    model_name = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = QuestionAnsweringArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model \n",
    "train_args = {\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"use_cached_eval_features\": True,\n",
    "    \"output_dir\": f\"outputs/{model_type}\",\n",
    "    \"best_model_dir\": f\"outputs/{model_type}/best_model\",\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"max_seq_length\": 128,\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"evaluate_during_training_steps\": 1000,\n",
    "    # \"wandb_project\": \"Question Answer Application\",\n",
    "    # \"wandb_kwargs\": {\"name\": model_name},\n",
    "    \"save_model_every_epoch\": False,\n",
    "    \"save_eval_checkpoints\": False,\n",
    "    \"n_best_size\": 3,\n",
    "    # \"use_early_stopping\": True,\n",
    "    # \"early_stopping_metric\": \"mcc\",\n",
    "    # \"n_gpu\": 2,\n",
    "    # \"manual_seed\": 4,\n",
    "    \"use_multiprocessing\": False,\n",
    "    \"train_batch_size\": 128,\n",
    "    \"eval_batch_size\": 64,\n",
    "    # \"config\": {\n",
    "    #     \"output_hidden_states\": True\n",
    "    # },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Andrea FS\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Andrea FS\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the QuestionAnsweringModel\n",
    "model = QuestionAnsweringModel(\n",
    "    model_type=model_type,\n",
    "    model_name=model_name,\n",
    "    args=train_args,\n",
    "    use_cuda=False  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 21/21 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\n",
      "add example index and unique id: 100%|██████████| 21/21 [00:00<00:00, 489335.47it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a8fcff53744adc91241fe505bfbcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f1659497c64f23ae88f162de1cac1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ede8929df704c96b9d9d97d13a2fdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a6887fa4584ac690a4bd063c5d3a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0509b6902e2478da94277d106717168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1b4db267394f128c9008856820ea4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0b723db2434a5fb1f97849cc771333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113a5b1e7f3844098fe6e3ce8c28a428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f2e10fc580468cb9d3b90f8d15c102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0a0e99b53947d094e37d2a5969ec2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 5:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ceb8ef0366049619e2c9a0f81fa7103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " {'global_step': [1, 2, 3, 4, 5],\n",
       "  'correct': [0, 0, 0, 0, 0],\n",
       "  'similar': [2, 2, 2, 2, 2],\n",
       "  'incorrect': [1, 1, 1, 1, 1],\n",
       "  'train_loss': [4.842485427856445,\n",
       "   4.839571475982666,\n",
       "   4.309649467468262,\n",
       "   3.901332378387451,\n",
       "   3.600252866744995],\n",
       "  'eval_loss': [-0.1390146166086197,\n",
       "   -0.11305144429206848,\n",
       "   -0.09287238121032715,\n",
       "   -0.08376660943031311,\n",
       "   -0.08095118403434753]})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train_model(train, eval_data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8e3722717f4826895a8394788cbe7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "result, texts = model.eval_model(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'correct': 0, 'similar': 2, 'incorrect': 1, 'eval_loss': -0.08095118403434753}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the model\n",
    "to_predict = [\n",
    "    {\n",
    "        \"context\": \"Canadian Tire Corporation, is a group of companies that includes a Retail segment, a Financial Services division and CT REIT. Our retail business is led by Canadian Tire, which was founded in 1922 and provides Canadians with products for life in Canada across its Living, Playing, Fixing, Automotive and Seasonal & Gardening divisions.\",\n",
    "        \"qas\": [\n",
    "            {\n",
    "            \"id\": \"q16\",\n",
    "            \"question\": \"What about Canadian Tire Corporation?\"\n",
    "            }\n",
    "        ]  \n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 1022.50it/s]\n",
      "\n",
      "\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 16131.94it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cf360bc42445bcbf7203dd020beb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Prediction:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answers, probabilities = model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 'q16', 'answer': ['is a group of companies that includes a Retail segment, a Financial Services division and CT REIT. Our retail business is led by Canadian Tire, which was founded in 1922 and provides Canadians with products for life in Canada across its Living, Playing, Fixing, Automot', 'is a group of companies that includes a Retail segment, a Financial Services division and CT REIT. Our retail business is led by Canadian Tire, which was founded in 1922 and provides Canadians with products for life in Canada across its Living, Playing, Fixing, Automotive and Seasonal &', ', Playing, Fixing, Automot']}]\n"
     ]
    }
   ],
   "source": [
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model\n",
    "\n",
    "def ask_question_in_notebook(model):\n",
    "    \"\"\"\n",
    "    Ask a question to the trained QA model and receive an answer interactively in a Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        model (QuestionAnsweringModel): The trained QA model.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the question and the answer in the notebook.\n",
    "    \"\"\"\n",
    "    # Input the context and question dynamically\n",
    "    context = input(\"Enter the context (paragraph containing the answer):\\n\")\n",
    "    question = input(\"\\nEnter your question:\\n\")\n",
    "    \n",
    "    # Prepare the prediction input\n",
    "    to_predict = [\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"qas\": [\n",
    "                {\n",
    "                    \"id\": \"1\",  # Dummy ID\n",
    "                    \"question\": question\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Get predictions\n",
    "    answers, _ = model.predict(to_predict)\n",
    "    \n",
    "    # Extract and display the answer\n",
    "    answer = answers[0]['answer'][0] if answers and 'answer' in answers[0] else \"No answer found\"\n",
    "    print(\"\\n--- Model's Answer ---\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_trained_type = \"bert\"\n",
    "model_trained_name = \"outputs/bert/best_model\"\n",
    "model_trained = QuestionAnsweringModel( model_type=model_trained_type, model_name=model_trained_name, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 339.10it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 339.10it/s]\n",
      "add example index and unique id: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Running Prediction: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model's Answer ---\n",
      "Question: What is Canadian Tire?\n",
      "Answer: empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question_in_notebook(model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta-squad\n",
    "\n",
    "Based on deepset/tinyroberta-squad2\n",
    "\n",
    "https://huggingface.co/deepset/tinyroberta-squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1854\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1853\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1854\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:105\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# We keep only the field we are interested in\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    106\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_json(io\u001b[38;5;241m.\u001b[39mStringIO(ujson_dumps(dataset)))\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultDataCollator\n\u001b[1;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed_train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed_test.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m     16\u001b[0m     questions \u001b[38;5;241m=\u001b[39m [q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqas\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2154\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1007\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1741\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1739\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1897\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[0;32m   1896\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load and preprocess your SQuAD-formatted dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"processed_train.json\", \"test\": \"processed_test.json\"}, field=\"data\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q[\"question\"] for q in examples[\"qas\"]]\n",
    "    contexts = [examples[\"context\"] for _ in examples[\"qas\"]]\n",
    "    answers = [qa[\"answers\"][0] for qa in examples[\"qas\"]]\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Map tokens to answer positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Default values\n",
    "        start_positions.append(cls_index)\n",
    "        end_positions.append(cls_index)\n",
    "\n",
    "        # Check if there's an answer\n",
    "        answer = answers[i]\n",
    "        if len(answer) > 0:\n",
    "            start_char = answer[\"answer_start\"]\n",
    "            end_char = start_char + len(answer[\"text\"])\n",
    "            \n",
    "            token_start_index = 0\n",
    "            token_end_index = len(offsets) - 1\n",
    "\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions[i] = token_start_index - 1\n",
    "\n",
    "            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions[i] = token_end_index + 1\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'topic', 'qas'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.json\", \"test\": \"test.json\"})\n",
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for entry in data:\n",
    "        context = entry[\"context\"]\n",
    "        for qa in entry[\"qas\"]:\n",
    "            processed_data.append({\n",
    "                \"context\": context,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": qa[\"answers\"]\n",
    "            })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Preprocess train and test datasets\n",
    "train_data = preprocess_data(\"train.json\")\n",
    "test_data = preprocess_data(\"test.json\")\n",
    "\n",
    "# Save the processed data\n",
    "with open(\"processed_train.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "with open(\"processed_test.json\", \"w\") as f:\n",
    "    json.dump(test_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embbed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data in a simplified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
      "--- Model's Answer ---\n",
      "Question: What is Canadian Tire?\n",
      "Answer: empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ask_question_in_notebook(model_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta-squad\n",
    "\n",
    "Based on deepset/tinyroberta-squad2\n",
    "\n",
    "https://huggingface.co/deepset/tinyroberta-squad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1854\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1853\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 1854\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:105\u001b[0m, in \u001b[0;36mJson._generate_tables\u001b[1;34m(self, files)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# We keep only the field we are interested in\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfield\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    106\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_json(io\u001b[38;5;241m.\u001b[39mStringIO(ujson_dumps(dataset)))\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultDataCollator\n\u001b[1;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed_train.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed_test.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n\u001b[0;32m     16\u001b[0m     questions \u001b[38;5;241m=\u001b[39m [q[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqas\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2154\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2165\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m    996\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[1;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[0;32m   1007\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1741\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[0;32m   1739\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[1;32m-> 1741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\datasets\\builder.py:1897\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[0;32m   1895\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[0;32m   1896\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1897\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[1;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load and preprocess your SQuAD-formatted dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"processed_train.json\", \"test\": \"processed_test.json\"}, field=\"data\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    questions = [q[\"question\"] for q in examples[\"qas\"]]\n",
    "    contexts = [examples[\"context\"] for _ in examples[\"qas\"]]\n",
    "    answers = [qa[\"answers\"][0] for qa in examples[\"qas\"]]\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Map tokens to answer positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Default values\n",
    "        start_positions.append(cls_index)\n",
    "        end_positions.append(cls_index)\n",
    "\n",
    "        # Check if there's an answer\n",
    "        answer = answers[i]\n",
    "        if len(answer) > 0:\n",
    "            start_char = answer[\"answer_start\"]\n",
    "            end_char = start_char + len(answer[\"text\"])\n",
    "            \n",
    "            token_start_index = 0\n",
    "            token_end_index = len(offsets) - 1\n",
    "\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions[i] = token_start_index - 1\n",
    "\n",
    "            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions[i] = token_end_index + 1\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# Preprocess the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['context', 'topic', 'qas'],\n",
      "    num_rows: 15\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files={\"train\": \"train.json\", \"test\": \"test.json\"})\n",
    "print(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for entry in data:\n",
    "        context = entry[\"context\"]\n",
    "        for qa in entry[\"qas\"]:\n",
    "            processed_data.append({\n",
    "                \"context\": context,\n",
    "                \"question\": qa[\"question\"],\n",
    "                \"answers\": qa[\"answers\"]\n",
    "            })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Preprocess train and test datasets\n",
    "train_data = preprocess_data(\"train.json\")\n",
    "test_data = preprocess_data(\"test.json\")\n",
    "\n",
    "# Save the processed data\n",
    "with open(\"processed_train.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "with open(\"processed_test.json\", \"w\") as f:\n",
    "    json.dump(test_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embbed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data in a simplified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Canadian Tire?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_name = \"outputs/bert/best_model\"  # Path to your fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to test the model\n",
    "def answer_question(question, context):\n",
    "    # Tokenize the input question and context\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question, context, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the start and end of the answer in the input_ids\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    # Convert token IDs back to words\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index])\n",
    "    )\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "# Test the model with an example\n",
    "context = \"\"\"\n",
    "Canadian Tire is a retail company in Canada offering a wide range of products, \n",
    "including automotive, home, and sporting goods. The company operates through multiple divisions, \n",
    "including Canadian Tire Retail (CTR), Financial Services, and Mark's Work Wearhouse.\n",
    "\"\"\"\n",
    "question = \"What is Canadian Tire?\"\n",
    "\n",
    "answer = answer_question(question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to 'chunked_data.json'\n"
      "Question: What is Canadian Tire?\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_name = \"outputs/bert/best_model\"  # Path to your fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to test the model\n",
    "def answer_question(question, context):\n",
    "    # Tokenize the input question and context\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question, context, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the start and end of the answer in the input_ids\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "\n",
    "    # Convert token IDs back to words\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index])\n",
    "    )\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "# Test the model with an example\n",
    "context = \"\"\"\n",
    "Canadian Tire is a retail company in Canada offering a wide range of products, \n",
    "including automotive, home, and sporting goods. The company operates through multiple divisions, \n",
    "including Canadian Tire Retail (CTR), Financial Services, and Mark's Work Wearhouse.\n",
    "\"\"\"\n",
    "question = \"What is Canadian Tire?\"\n",
    "\n",
    "answer = answer_question(question, context)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data saved to 'chunked_data.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from itertools import count\n",
    "\n",
    "# Load the original JSON file\n",
    "with open(\"processed_text.json\", \"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "# Initialize counters and storage for transformed data\n",
    "chunked_data = []\n",
    "unique_id = count(1)  # A counter to generate unique IDs\n",
    "\n",
    "# Process each section in the document_sections\n",
    "for section in original_data.get(\"document_sections\", []):\n",
    "    title = section.get(\"title\", \"\")\n",
    "    preprocessed_text = section.get(\"preprocessed_text\", \"\")\n",
    "    \n",
    "    # Split the preprocessed_text into smaller chunks (e.g., paragraphs or sentences)\n",
    "    chunks = preprocessed_text.split(\"\\n\\n\") if \"\\n\\n\" in preprocessed_text else preprocessed_text.split(\". \")\n",
    "    \n",
    "    # Create a new dictionary entry for each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.strip()  # Remove leading/trailing whitespace\n",
    "        if chunk:  # Ensure the chunk isn't empty\n",
    "            chunked_data.append({\n",
    "                \"id\": next(unique_id),\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "# Save the transformed data to a new JSON file\n",
    "with open(\"chunked_data.json\", \"w\") as f:\n",
    "    json.dump(chunked_data, f, indent=4)\n",
    "\n",
    "print(f\"Transformed data saved to 'chunked_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Andrea FS\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load transformed data\n",
    "with open(\"chunked_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Compute embeddings for each chunk\n",
    "for item in data:\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, data, embedding_model):\n",
    "    question_embedding = embedding_model.encode(question)\n",
    "\n",
    "    # Calculate cosine similarity between question and dataset chunks\n",
    "    similarities = [\n",
    "        (item[\"id\"], cosine_similarity([question_embedding], [item[\"embedding\"]])[0][0])\n",
    "        for item in data\n",
    "    ]\n",
    "    # Get the most relevant chunk\n",
    "    best_match = max(similarities, key=lambda x: x[1])\n",
    "    context = next(item[\"text\"] for item in data if item[\"id\"] == best_match[0])\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: canadian tire corporation limited tsx ctca tsx ctc ctc group company includes retail segment financial service division ct reit retail business led canadian tire founded 1922 provides canadian product life canada across living playing fixing automotive seasonal gardening division party city partsource gas key part canadian tire network retail segment also includes mark leading source casual industrial wear pro hockey life hockey specialty store catering elite player sportchek hockey expert sport expert atmosphere offer best active wear brand company close 1700 retail gasoline outlet supported strengthened ctc financial service division ten thousand people employed across canada around world ctc local dealer franchisees petroleum retailer addition ctc owns operates helly hansen leading technical outdoor brand based oslo norway information\n",
      "Q: What is Canadian Tire?\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def chat_with_model(question, data, embedding_model, qa_model, tokenizer):\n",
    "    # Retrieve the most relevant context\n",
    "    context = retrieve_context(question, data, embedding_model)\n",
    "    print(f\"Retrieved Context: {context}\")  # Debugging step to view retrieved context\n",
    "\n",
    "    # Answer the question using the QA model\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question, context,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    outputs = qa_model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Extract answer\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index])\n",
    "    )\n",
    "    return answer.strip()\n",
    "\n",
    "# Test the chatbot\n",
    "question = \"What is Canadian Tire?\"\n",
    "answer = chat_with_model(question, data, embedding_model, model, tokenizer)\n",
    "print(f\"Q: {question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
    "import json\n",
    "from itertools import count\n",
    "\n",
    "# Load the original JSON file\n",
    "with open(\"processed_text.json\", \"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "# Initialize counters and storage for transformed data\n",
    "chunked_data = []\n",
    "unique_id = count(1)  # A counter to generate unique IDs\n",
    "\n",
    "# Process each section in the document_sections\n",
    "for section in original_data.get(\"document_sections\", []):\n",
    "    title = section.get(\"title\", \"\")\n",
    "    preprocessed_text = section.get(\"preprocessed_text\", \"\")\n",
    "    \n",
    "    # Split the preprocessed_text into smaller chunks (e.g., paragraphs or sentences)\n",
    "    chunks = preprocessed_text.split(\"\\n\\n\") if \"\\n\\n\" in preprocessed_text else preprocessed_text.split(\". \")\n",
    "    \n",
    "    # Create a new dictionary entry for each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk = chunk.strip()  # Remove leading/trailing whitespace\n",
    "        if chunk:  # Ensure the chunk isn't empty\n",
    "            chunked_data.append({\n",
    "                \"id\": next(unique_id),\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "# Save the transformed data to a new JSON file\n",
    "with open(\"chunked_data.json\", \"w\") as f:\n",
    "    json.dump(chunked_data, f, indent=4)\n",
    "\n",
    "print(f\"Transformed data saved to 'chunked_data.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Andrea FS\\OneDrive\\Lambton college\\second term\\new_nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Andrea FS\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load transformed data\n",
    "with open(\"chunked_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Compute embeddings for each chunk\n",
    "for item in data:\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, data, embedding_model):\n",
    "    question_embedding = embedding_model.encode(question)\n",
    "\n",
    "    # Calculate cosine similarity between question and dataset chunks\n",
    "    similarities = [\n",
    "        (item[\"id\"], cosine_similarity([question_embedding], [item[\"embedding\"]])[0][0])\n",
    "        for item in data\n",
    "    ]\n",
    "    # Get the most relevant chunk\n",
    "    best_match = max(similarities, key=lambda x: x[1])\n",
    "    context = next(item[\"text\"] for item in data if item[\"id\"] == best_match[0])\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context: canadian tire corporation limited tsx ctca tsx ctc ctc group company includes retail segment financial service division ct reit retail business led canadian tire founded 1922 provides canadian product life canada across living playing fixing automotive seasonal gardening division party city partsource gas key part canadian tire network retail segment also includes mark leading source casual industrial wear pro hockey life hockey specialty store catering elite player sportchek hockey expert sport expert atmosphere offer best active wear brand company close 1700 retail gasoline outlet supported strengthened ctc financial service division ten thousand people employed across canada around world ctc local dealer franchisees petroleum retailer addition ctc owns operates helly hansen leading technical outdoor brand based oslo norway information\n",
      "Q: What is Canadian Tire?\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def chat_with_model(question, data, embedding_model, qa_model, tokenizer):\n",
    "    # Retrieve the most relevant context\n",
    "    context = retrieve_context(question, data, embedding_model)\n",
    "    print(f\"Retrieved Context: {context}\")  # Debugging step to view retrieved context\n",
    "\n",
    "    # Answer the question using the QA model\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question, context,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    outputs = qa_model(**inputs)\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Extract answer\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "    start_index = torch.argmax(start_scores)\n",
    "    end_index = torch.argmax(end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[start_index:end_index])\n",
    "    )\n",
    "    return answer.strip()\n",
    "\n",
    "# Test the chatbot\n",
    "question = \"What is Canadian Tire?\"\n",
    "answer = chat_with_model(question, data, embedding_model, model, tokenizer)\n",
    "print(f\"Q: {question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
